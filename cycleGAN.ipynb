{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish reading all data...\n",
      "Iteration 0\n",
      "Time usage: 0:00:04...\n",
      "\n",
      "Iteration 100\n",
      "Time usage: 0:01:09...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from ops import *\n",
    "from datetime import timedelta, datetime\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define dataset class\n",
    "Data = Dataset()\n",
    "\n",
    "# define placeholder variables\n",
    "input_A = tf.placeholder(tf.float32, [BATCH_SIZE, 256, 256, 3], name='input_A')\n",
    "input_B = tf.placeholder(tf.float32, [BATCH_SIZE, 256, 256, 3], name='input_B')\n",
    "fake_pool_A = tf.placeholder(tf.float32, [None, 256, 256, 3], name=\"fake_pool_A\")\n",
    "fake_pool_B = tf.placeholder(tf.float32, [None, 256, 256, 3], name=\"fake_pool_B\")\n",
    "is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "learning_rate_g = tf.placeholder(tf.float32, name='learning_rate_g')\n",
    "learning_rate_d = tf.placeholder(tf.float32, name='learning_rate_d')\n",
    "\n",
    "\n",
    "# real_A to fake_B\n",
    "gen_B = generator(input_A, is_train=is_train, name='generator_AtoB')\n",
    "dec_B_real = discriminator(input_B, is_train=is_train, name='discriminator_B')\n",
    "dec_B_fake = discriminator(gen_B, is_train=is_train, reuse=True, name='discriminator_B')\n",
    "\n",
    "# real_B to fake_A\n",
    "gen_A = generator(input_B, is_train=is_train, name='generator_BtoA')\n",
    "dec_A_real = discriminator(input_A, is_train=is_train, name='discriminator_A')\n",
    "dec_A_fake = discriminator(gen_A, is_train=is_train, reuse=True, name='discriminator_A')\n",
    "\n",
    "# fake_B to real_A\n",
    "gen_A_from_fake_B = generator(gen_B, is_train=is_train, reuse=True, name='generator_BtoA')\n",
    "\n",
    "# fake_A to real_B\n",
    "gen_B_from_fake_A = generator(gen_A, is_train=is_train, reuse=True, name='generator_AtoB')\n",
    "\n",
    "# discriminate fake_pool_A\n",
    "dec_A_pool_fake = discriminator(fake_pool_A, is_train=is_train, reuse=True, name='discriminator_A')\n",
    "\n",
    "# discriminate fake_pool_B\n",
    "dec_B_pool_fake = discriminator(fake_pool_B, is_train=is_train, reuse=True, name='discriminator_B')\n",
    "\n",
    "\n",
    "\n",
    "# add summary\n",
    "temp1 = (input_A + 1)*127.5\n",
    "temp2 = (gen_A + 1)*127.5\n",
    "temp3 = (input_B + 1)*127.5\n",
    "temp4 = (gen_B + 1)*127.5\n",
    "temp5 = (gen_A_from_fake_B + 1)*127.5\n",
    "temp6 = (gen_B_from_fake_A + 1)*127.5\n",
    "\n",
    "tempA = tf.concat((temp1, temp4, temp5), axis=2)\n",
    "tempB = tf.concat((temp3, temp2, temp6), axis=2)\n",
    "\n",
    "tf.summary.image('results_A/toB/backA', tf.cast(tempA, tf.uint8), max_outputs=1)\n",
    "tf.summary.image('results_B/toA/backB', tf.cast(tempB, tf.uint8), max_outputs=1)\n",
    "\n",
    "\n",
    "# define cycle loss\n",
    "cycle_loss_A = cycle_consistency_loss(gen_A_from_fake_B, input_A)\n",
    "cycle_loss_B = cycle_consistency_loss(gen_B_from_fake_A, input_B)\n",
    "cycle_loss = cycle_loss_A + cycle_loss_B\n",
    "\n",
    "# define gan loss \n",
    "loss_gen_A_1 = generator_loss(dec_A_fake, use_lsgan=True)\n",
    "loss_gen_B_1 = generator_loss(dec_B_fake, use_lsgan=True)\n",
    "lamda = 10\n",
    "\n",
    "# define generator loss\n",
    "loss_gen_A = loss_gen_A_1 + lamda * cycle_loss\n",
    "loss_gen_B = loss_gen_B_1 + lamda * cycle_loss\n",
    "\n",
    "# define discriminator loss\n",
    "loss_dec_A = discriminator_loss(dec_A_real, dec_A_pool_fake, use_lsgan=True)\n",
    "loss_dec_B = discriminator_loss(dec_B_real, dec_B_pool_fake, use_lsgan=True)\n",
    "\n",
    "\n",
    "# add summary\n",
    "tf.summary.scalar('loss_dec_A', loss_dec_A)\n",
    "tf.summary.scalar('loss_dec_B', loss_dec_B)\n",
    "tf.summary.scalar('regular_gan_loss_A', loss_gen_A_1)\n",
    "tf.summary.scalar('regular_gan_loss_B', loss_gen_B_1)\n",
    "tf.summary.scalar('cycle_loss', cycle_loss)\n",
    "tf.summary.scalar('probability_real_A', tf.reduce_mean(dec_A_real))\n",
    "tf.summary.scalar('probability_fake_A', tf.reduce_mean(dec_A_fake))\n",
    "tf.summary.scalar('probability_real_B', tf.reduce_mean(dec_B_real))\n",
    "tf.summary.scalar('probability_fake_B', tf.reduce_mean(dec_B_fake))\n",
    "\n",
    "\n",
    "\n",
    "# retrieve variables\n",
    "var_g_A = [item for item in tf.trainable_variables() if item.name.startswith('generator_AtoB')]\n",
    "var_d_A = [item for item in tf.trainable_variables() if item.name.startswith('discriminator_A')]\n",
    "var_g_B = [item for item in tf.trainable_variables() if item.name.startswith('generator_BtoA')]\n",
    "var_d_B = [item for item in tf.trainable_variables() if item.name.startswith('discriminator_B')]\n",
    "\n",
    "# define optimizer function\n",
    "with tf.name_scope('training'):\n",
    "    op_gen_A = optimizer(loss_gen_A, var_g_A, lr=learning_rate_g, name='op_gen_A')\n",
    "    op_dec_A = optimizer(loss_dec_A, var_d_A, lr=learning_rate_d, name='op_dec_A')\n",
    "    op_gen_B = optimizer(loss_gen_B, var_g_B, lr=learning_rate_g, name='op_gen_B')\n",
    "    op_dec_B = optimizer(loss_dec_B, var_d_B, lr=learning_rate_d, name='op_dec_B')\n",
    "\n",
    "\n",
    "# define session\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# initialize saver to store model \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# add to tensorboard\n",
    "writer = tf.summary.FileWriter('Tensorboard/apple2orange_test8')\n",
    "writer.add_graph(session.graph)\n",
    "merge = tf.summary.merge_all()\n",
    "\n",
    "t1 = time.time()\n",
    "num_fake_inputs_A = 0\n",
    "num_fake_inputs_B = 0\n",
    "temp_fake_pool_A = None\n",
    "temp_fake_pool_B = None\n",
    "\n",
    "# begin training\n",
    "for i in range(100001):\n",
    "    \n",
    "    if i < 5e4:\n",
    "        lr_g = 0.0002\n",
    "        lr_d = 0.0002\n",
    "    else:\n",
    "        lr_g = 0.0002 - 0.0002*(i - 5e4)/5e4\n",
    "        lr_d = 0.0002 - 0.0001*(i - 5e4)/5e4\n",
    "        \n",
    "\n",
    "    d_A, d_B = Data.next_batch()\n",
    "    \n",
    "    feed_dict={input_A:d_A, input_B:d_B, is_train:True}\n",
    "    fake_A_temp, fake_B_temp = session.run([gen_A, gen_B], feed_dict=feed_dict)\n",
    "    temp_fake_pool_A = fake_image_pool(num_fake_inputs_A, fake_A_temp, temp_fake_pool_A)\n",
    "    temp_fake_pool_B = fake_image_pool(num_fake_inputs_B, fake_B_temp, temp_fake_pool_B)\n",
    "    num_fake_inputs_A += 1\n",
    "    num_fake_inputs_B += 1\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # write to tensorboard\n",
    "    if i%10 == 0: \n",
    "        feed_dict = {input_A:d_A, input_B:d_B, is_train:True, learning_rate_g:lr_g, learning_rate_d:lr_d,\n",
    "                    fake_pool_A:temp_fake_pool_A, fake_pool_B:temp_fake_pool_B}\n",
    "                    \n",
    "        im, result = session.run([gen_A_from_fake_B, merge], feed_dict=feed_dict)  \n",
    "        writer.add_summary(result, i)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Optimizing the G_B network\n",
    "    _, temp_B = session.run([op_gen_B, gen_B], feed_dict={input_A:d_A, input_B:d_B, is_train:True, \n",
    "                            learning_rate_g:lr_g, learning_rate_d:lr_d})\n",
    "    \n",
    "    temp_fake_pool_B = fake_image_pool(num_fake_inputs_B, temp_B, temp_fake_pool_B)\n",
    "    num_fake_inputs_B += 1\n",
    "    \n",
    "\n",
    "    # Optimizing the D_B network\n",
    "    session.run([op_dec_B],feed_dict={input_B:d_B, is_train:True, learning_rate_g:lr_g, learning_rate_d:lr_d, \n",
    "                                    fake_pool_B:temp_fake_pool_B})\n",
    "\n",
    "    # Optimizing the G_A network\n",
    "    _, temp_A = session.run([op_gen_A, gen_A], feed_dict={input_A:d_A, input_B:d_B, is_train:True, \n",
    "                            learning_rate_g:lr_g, learning_rate_d:lr_d})\n",
    "\n",
    "    temp_fake_pool_A = fake_image_pool(num_fake_inputs_A, temp_A, temp_fake_pool_A)\n",
    "    num_fake_inputs_A += 1\n",
    "    \n",
    "\n",
    "    # Optimizing the D_A network\n",
    "    session.run([op_dec_A],feed_dict={input_A:d_A, is_train:True, learning_rate_g:lr_g, learning_rate_d:lr_d, \n",
    "                                    fake_pool_A:temp_fake_pool_A})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "\n",
    "    if i%100 == 0:\n",
    "        d_A, d_B = Data.get_random_test_batch(BATCH_SIZE)        \n",
    "        feed_dict = {input_A:d_A, input_B:d_B, is_train:True}\n",
    "\n",
    "        t2 = time.time()\n",
    "        time_dif = t2 - t1\n",
    "        print('Iteration {}'.format(i))\n",
    "        print('Time usage: {}...'.format(timedelta(seconds=int(time_dif))))\n",
    "        print()\n",
    "\n",
    "        gen_A_image, gen_B_image = session.run([gen_A, gen_B], feed_dict=feed_dict)\n",
    "        images = np.concatenate((d_A, gen_B_image, d_B, gen_A_image), axis=0)\n",
    "        deprocess_and_save_result(images, i, )\n",
    "        \n",
    "    if i%10000 == 0 and i > 1:\n",
    "        path = os.path.join(os.getcwd(), 'model/MODEL.ckpt')\n",
    "        saver.save(session, path, global_step=i)\n",
    "        print('temporal model saved.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('finished.')\n",
    "# path = os.path.join(os.getcwd(), 'model/MODEL.ckpt')\n",
    "# saver.save(session, path)\n",
    "# print('model saved.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
